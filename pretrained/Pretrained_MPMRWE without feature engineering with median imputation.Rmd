---
title: "MPMRWE-withoutFE-withmI-alt"
output: html_document
---

Load packages for project
```{r setup, include=FALSE}
options(width = 120)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
library(tidyverse)
library(skimr)
library(tidymodels)
library(matrixStats)
library(vip)
#library(kernlab)
#library(doParallel)
library(discrim)
library(stacks)
#library(kknn)
```


# Loading data and data curation
```{r load_data}
## Synthetic data for analysis
dfraw <- read_csv("/users/home/candwei/pm_master/cll_tim_compound_infec_n.csv") %>% 
  rename("id" = "...1") 
```

```{r}
df <- dfraw %>% 
  select(-c(id, Patient_id, INFEC_days_after_diag)) %>% 
  mutate(INFEC_after_diag_num = INFEC_after_diag) %>%
  mutate(INFEC_after_diag = factor(INFEC_after_diag, levels = c("1", "0")))

colnames(df)[166:169] <- c("NAT_min","NAT_max","NAT_sd","NAT_mean")
```


```{r}
set.seed(42)

# Create random samples of the rows with 20% set aside for testing
df_split <- df %>% 
  initial_split(prop = 0.8)

# Extract the datasets
df_other <- training(df_split)
df_test  <- testing(df_split)

# For the non testing set we create a 75/25 train/validation split
# The original data is then split into 60/20/20 proportions for training, validation and testing
df_val <- vfold_cv(df_other, v = 5)
```

# Define models

## Set overall recipe
```{r}
metric <- metric_set(roc_auc)

ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()

size_grid = 50

cll_rec <-
  recipe(INFEC_after_diag ~ ., data = df_other) %>%
  step_rm(INFEC_after_diag_num) %>%
  step_dummy(ends_with("nr"), one_hot = TRUE) %>%
  step_zv(all_predictors()) %>%           # Remove variables with no variation
  step_impute_median(all_predictors()) %>%
  step_normalize(all_predictors())
  

cll_wflow <-
  workflow() %>%
  add_recipe(cll_rec)
```


## LR
```{r}
lr_res_wfemi =   readRDS("lr_res_wfemi")
```

We can print the metrics from the model training using the collect_metrics command.
```{r}
lr_res_wfemi %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(lr_res_wfemi)

```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
lr_best_wfemi <- 
  lr_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

lr_best_wfemi

# Alternatively the sorting and slicing may be done using
# show_best(lr_res)
```

Finally we can plot the ROC for the best model
```{r}
lr_auc_wfemi <- 
  lr_res_wfemi %>% 
  collect_predictions(parameters = lr_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Elastic Net")

autoplot(lr_auc_wfemi)
```

## RF
```{r}
rf_res_wfemi = readRDS("rf_res_wfemi")
```

Print the top performing models
```{r}
show_best(rf_res_wfemi)
```

We can plot the performance against the tuning parameters
```{r}
autoplot(rf_res_wfemi)
```

ROC for the best model
```{r}
rf_best_wfemi <- 
  rf_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

rf_best_wfemi

rf_auc_wfemi <- 
  rf_res_wfemi %>% 
  collect_predictions(parameters = rf_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Random forrest")

autoplot(rf_auc_wfemi)
```


## SVM
```{r}
svm_res_wfemi = readRDS("svm_res_wfemi")
```

We can print the metrics from the model training using the collect_metrics command.
```{r}
svm_res_wfemi %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(svm_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
svm_best_wfemi <- 
  svm_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#svm_best
show_best(svm_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
svm_auc_wfemi <- 
  svm_res_wfemi %>% 
  collect_predictions(parameters = svm_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "SVM")

autoplot(svm_auc_wfemi)
```

## SVM with radial basis
```{r}
svm_r_res_wfemi = readRDS("svm_r_res_wfemi")
```
We can print the metrics from the model training using the collect_metrics command.
```{r}
svm_r_res_wfemi %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(svm_r_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
svm_r_best_wfemi <- 
  svm_r_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#s best svm_r
show_best(svm_r_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
svm_r_auc_wfemi <- 
  svm_r_res_wfemi %>% 
  collect_predictions(parameters = svm_r_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "SVM radial")

autoplot(svm_r_auc_wfemi)
```

## extrem gradient boost
```{r}
xgb_res_wfemi = readRDS("xgb_res_wfemi")

```

We can print the metrics from the model training using the collect_metrics command.
```{r}
xgb_res_wfemi %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(xgb_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
xgb_best_wfemi <- 
  xgb_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#svm_best
show_best(xgb_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
xgb_auc_wfemi <- 
  xgb_res_wfemi %>% 
  collect_predictions(parameters = xgb_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "xgboost")

autoplot(xgb_auc_wfemi)
```


## Multilayer perceptron
```{r}
mlp_res_wfemi = readRDS("mlp_res_wfemi")

```

We can print the metrics from the model training using the collect_metrics command.
```{r}
mlp_res_wfemi %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(mlp_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
mlp_best_wfemi <- 
  mlp_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#mlp_best
show_best(mlp_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
mlp_auc_wfemi <- 
  mlp_res_wfemi %>% 
  collect_predictions(parameters = mlp_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Multilayer Perceptron")

autoplot(mlp_auc_wfemi)
```

## Naive Bayes
```{r}
nb_res_wfemi = readRDS("nb_res_wfemi")
```

```{r}
nb_res_wfemi %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(nb_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
nb_best_wfemi <- 
  nb_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

show_best(nb_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
nb_auc_wfemi <- 
  nb_res_wfemi %>% 
  collect_predictions(parameters = nb_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "naive Bayes")

autoplot(nb_auc_wfemi)
```

# Bagged tree
```{r}
bag_res_wfemi = readRDS("bag_res_wfemi")

```


We can print the metrics from the model training using the collect_metrics command.
```{r}
bag_res_wfemi %>%
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(bag_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
bag_best_wfemi <- 
  bag_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#bag best
show_best(bag_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
bag_auc_wfemi <- 
  bag_res_wfemi %>% 
  collect_predictions(parameters = bag_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Bagged trees")

autoplot(bag_auc_wfemi)
```
# decision tree
```{r}
dec_res_wfemi = readRDS("dec_res_wfemi")
```


We can print the metrics from the model training using the collect_metrics command.
```{r}
dec_res_wfemi %>%
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(dec_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
dec_best_wfemi <- 
  dec_res_wfemi %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#bag best
show_best(dec_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
dec_auc_wfemi <- 
  dec_res_wfemi %>% 
  collect_predictions(parameters = dec_best_wfemi) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Decision tree")

autoplot(dec_auc_wfemi)
```

# Multivariate adaptive regression splines
```{r}
mars_res_wfemi = readRDS("mars_res_wfemi")
```


We can print the metrics from the model training using the collect_metrics command.
```{r}
mars_res_wfemi %>%
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(mars_res_wfemi)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
mars_best_wfemi <-
  mars_res_wfemi %>%
  collect_metrics() %>%
  arrange(-mean) %>%
  dplyr::slice(1)

#bag best
show_best(mars_res_wfemi)
```

Finally we can plot the ROC for the best model
```{r}
mars_auc_wfemi <-
  mars_res_wfemi %>%
  collect_predictions(parameters = mars_best_wfemi) %>%
  roc_curve(INFEC_after_diag, .pred_1) %>%
  mutate(model = "Multivariate adaptive regression splines")

autoplot(mars_auc_wfemi)
```

# Last model
This section demonstrates how to retrain the model on the combined training and validation set, using the optimal tuning parameters, and evaluating the performance on the test set.

## Metrics to report
```{r}
multi_metric = metric_set(accuracy,mcc, bal_accuracy, detection_prevalence,f_meas,j_index,kap,
                          npv, ppv, sensitivity,specificity, average_precision,classification_cost,
                          gain_capture,mn_log_loss,pr_auc,roc_auc)
```

## LR
```{r}
# Create model with best parameter
lr_final_mod_wfemi <- logistic_reg(penalty = lr_best_wfemi$penalty, mixture = lr_best_wfemi$mixture) %>% 
  set_engine("glmnet")

# Create final workflow
lr_final_workflow_wfemi <- workflow() %>% 
 add_model(lr_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
lr_last_fit_wfemi <- last_fit(lr_final_workflow_wfemi, split = df_split, metrics = multi_metric)
lr_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```

## RF
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
rf_final_mod_wfemi <- 
  rand_forest(mtry = rf_best_wfemi$mtry, min_n = rf_best_wfemi$min_n, trees = rf_best_wfemi$trees) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

# Create final workflow
rf_final_workflow_wfemi <- workflow() %>% 
 add_model(rf_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
rf_last_fit_wfemi <- last_fit(rf_final_workflow_wfemi, split = df_split, metrics = multi_metric)
rf_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```


## SVM
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
svm_final_mod_wfemi <- 
  svm_poly(cost = svm_best_wfemi$cost, degree = svm_best_wfemi$degree) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

# Create final workflow
svm_final_workflow_wfemi <- workflow() %>% 
  add_model(svm_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
svm_last_fit_wfemi <- last_fit(svm_final_workflow_wfemi, split = df_split, metrics = multi_metric)
svm_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```



## SVM with radial basis
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
svm_r_final_mod_wfemi <- 
  svm_rbf(cost = svm_r_best_wfemi$cost, rbf_sigma = svm_r_best_wfemi$rbf_sigma) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

# Create final workflow
svm_r_final_workflow_wfemi <- workflow() %>% 
 add_model(svm_r_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
svm_r_last_fit_wfemi <- last_fit(svm_r_final_workflow_wfemi, split = df_split, metrics = multi_metric)
svm_r_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```


## XGBOOSTING
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
xgb_final_mod_wfemi <- 
  boost_tree(min_n = xgb_best_wfemi$min_n,tree_depth = xgb_best_wfemi$tree_depth, learn_rate = xgb_best_wfemi$learn_rate,loss_reduction = xgb_best_wfemi$loss_reduction, trees=xgb_best_wfemi$trees) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

# Create final workflow
xgb_final_workflow_wfemi <- workflow() %>% 
 add_model(xgb_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
xgb_last_fit_wfemi <- last_fit(xgb_final_workflow_wfemi, split = df_split, metrics = multi_metric)
xgb_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```


## mlp
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
mlp_final_mod_wfemi <- 
  mlp(hidden_units = mlp_best_wfemi$hidden_units, penalty = mlp_best_wfemi$penalty, epochs = mlp_best_wfemi$epochs) %>% 
  set_engine("nnet", trace=0,MaxNWts=10000) %>% 
  set_mode("classification")

# Create final workflow
mlp_final_workflow_wfemi <- workflow() %>% 
 add_model(mlp_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
mlp_last_fit_wfemi <- last_fit(mlp_final_workflow_wfemi, split = df_split, metrics = multi_metric)
mlp_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```

##Bagged trees
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
bag_final_mod_wfemi <- 
   baguette::bag_tree(
           min_n = bag_best_wfemi$min_n,
           class_cost = bag_best_wfemi$class_cost)%>% 
  set_engine("C5.0") %>% 
  set_mode("classification")


# Create final workflow
bag_final_workflow_wfemi <- workflow() %>% 
 add_model(bag_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
bag_last_fit_wfemi <- last_fit(bag_final_workflow_wfemi, split = df_split, metrics = multi_metric)
bag_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```


## Naive Bayes
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
nb_final_mod_wfemi <- 
  naive_Bayes(smoothness = nb_best_wfemi$smoothness, Laplace = nb_best_wfemi$Laplace) %>% 
  set_engine("naivebayes") %>% 
  set_mode("classification")


# Create final workflow
nb_final_workflow_wfemi <- workflow() %>% 
 add_model(nb_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
nb_last_fit_wfemi <- last_fit(nb_final_workflow_wfemi, split = df_split, metrics = multi_metric)
nb_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```


## Decision tree
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
dec_final_mod_wfemi <- 
  decision_tree(
    min_n = dec_best_wfemi$min_n,
    cost_complexity = dec_best_wfemi$cost_complexity,
    tree_depth = dec_best_wfemi$tree_depth) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

# Create final workflow
dec_final_workflow_wfemi <- workflow() %>% 
 add_model(dec_final_mod_wfemi) %>% 
  add_recipe(cll_rec)

# Last fit
dec_last_fit_wfemi <- last_fit(dec_final_workflow_wfemi, split = df_split, metrics = multi_metric)
dec_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```

## MARS
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
mars_final_mod_wfemi <-
  mars(
    num_terms = mars_best_wfemi$num_terms,
    prune_method = mars_best_wfemi$prune_method,
    prod_degree = mars_best_wfemi$prod_degree) %>%
  set_engine("earth") %>%
  set_mode("classification")

# Create final workflow
mars_final_workflow_wfemi <- workflow() %>%
 add_model(mars_final_mod_wfemi) %>%
  add_recipe(cll_rec)

# Last fit
mars_last_fit_wfemi <- last_fit(mars_final_workflow_wfemi, split = df_split, metrics = multi_metric)
mars_last_fit_wfemi %>% collect_metrics() %>% select(1,3)
```

# Create stack
## Create stack
```{r}
cll_st_wfemi = readRDS("cll_st_wfemi")
```

## Visualize trade-off between number of members and performance
```{r}
autoplot(cll_st_wfemi)
```

## Other visualization showing trade-off vs performance
```{r}
autoplot(cll_st_wfemi, type = "members")
```

## Look at penalty 
```{r}
autoplot(cll_st_wfemi, type="weights")
```

## Check parameters
```{r}
collect_parameters(cll_st_wfemi,"lr_res_wfemi")
```

## Predicting new data
```{r}
cll_pred_wfemi <-
  df_test %>%
  bind_cols(predict(cll_st_wfemi,.,type = "prob"))
  
yardstick::roc_auc(
  cll_pred_wfemi,
  truth = INFEC_after_diag,
  contains(".pred_1")
)

cll_pred_wfemi %>% 
  pr_auc(INFEC_after_diag, .pred_1)

cll_pred_wfemi %>% 
  average_precision(INFEC_after_diag, .pred_1)

cll_pred_wfemi %>% 
  classification_cost(INFEC_after_diag, .pred_1)

cll_pred_wfemi %>% 
  gain_capture(INFEC_after_diag, .pred_1)

cll_pred_femi %>% 
  pr_auc(INFEC_after_diag, .pred_1)

cll_pred_wfemi %>% 
  mn_log_loss(INFEC_after_diag, .pred_1)
```

## compare with individual models
```{r}
cll_pred_wfemi <-
  df_test %>%
  select(INFEC_after_diag) %>%
  bind_cols(
    predict(
      cll_st_wfemi,
      df_test,
      type = "class",
      members = TRUE
    )
  )
```

## compare members with total
```{r}
map_dfr(
  setNames(colnames(cll_pred_wfemi),colnames(cll_pred_wfemi)),
  ~mean(cll_pred_wfemi$INFEC_after_diag == pull(cll_pred_wfemi, .x))
) %>%
  pivot_longer(c(everything(),-INFEC_after_diag))
```

## Callibration plots

## EN
### extract models from last fit
```{r}
lr_final_wfemi = lr_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_lr_wfemi = bind_cols(
  df_test,
  predict(lr_final_wfemi, df_test),
  predict(lr_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_lr_wfemi$bs =  (df_lr_wfemi$.pred_1 - df_lr_wfemi$INFEC_after_diag_num)^2
mean(df_lr_wfemi$bs)
```

## create calibration plot
```{r}
lr_cp = classifierplots::calibration_plot(df_lr_wfemi$INFEC_after_diag, df_lr_wfemi$.pred_1)
lr_cp
```
## RF
### extract models from last fit
```{r}
rf_final_wfemi = rf_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_rf_wfemi = bind_cols(
  df_test,
  predict(rf_final_wfemi, df_test),
  predict(rf_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_rf_wfemi$bs =  (df_rf_wfemi$.pred_1 - df_rf_wfemi$INFEC_after_diag_num)^2
mean(df_rf_wfemi$bs)
```

## create calibration plot
```{r}
rf_cp = classifierplots::calibration_plot(df_rf_wfemi$INFEC_after_diag, df_rf_wfemi$.pred_1)
rf_cp
```

## SVM
### extract models from last fit
```{r}
svm_final_wfemi = svm_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_svm_wfemi = bind_cols(
  df_test,
  predict(svm_final_wfemi, df_test),
  predict(svm_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_svm_wfemi$bs =  (df_svm_wfemi$.pred_1 - df_svm_wfemi$INFEC_after_diag_num)^2
mean(df_svm_wfemi$bs)
```

## create calibration plot
```{r}
svm_cp = classifierplots::calibration_plot(df_svm_wfemi$INFEC_after_diag, df_svm_wfemi$.pred_1)
svm_cp
```
## SVM with radient kernel basis
### extract models from last fit
```{r}
svm_r_final_wfemi = svm_r_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_svm_r_wfemi = bind_cols(
  df_test,
  predict(svm_r_final_wfemi, df_test),
  predict(svm_r_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_svm_r_wfemi$bs =  (df_svm_r_wfemi$.pred_1 - df_svm_r_wfemi$INFEC_after_diag_num)^2
mean(df_svm_r_wfemi$bs)
```

## create calibration plot
```{r}
svm_r_cp = classifierplots::calibration_plot(df_svm_r_wfemi$INFEC_after_diag, df_svm_r_wfemi$.pred_1)
svm_r_cp
```

## xgb
### extract models from last fit
```{r}
xgb_final_wfemi = xgb_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_xgb_wfemi = bind_cols(
  df_test,
  predict(xgb_final_wfemi, df_test),
  predict(xgb_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_xgb_wfemi$bs =  (df_xgb_wfemi$.pred_1 - df_xgb_wfemi$INFEC_after_diag_num)^2
mean(df_xgb_wfemi$bs)
```

## create calibration plot
```{r}
xgb_cp = classifierplots::calibration_plot(df_xgb_wfemi$INFEC_after_diag, df_xgb_wfemi$.pred_1)
xgb_cp
```

## Naive Bayes
### extract models from last fit
```{r}
nb_final_wfemi = nb_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_nb_wfemi = bind_cols(
  df_test,
  predict(nb_final_wfemi, df_test),
  predict(nb_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_nb_wfemi$bs =  (df_nb_wfemi$.pred_1 - df_nb_wfemi$INFEC_after_diag_num)^2
mean(df_nb_wfemi$bs)
```

## create calibration plot
```{r}
nb_cp = classifierplots::calibration_plot(df_nb_wfemi$INFEC_after_diag, df_nb_wfemi$.pred_1)
nb_cp
```

## Multilayer perceptron
### extract models from last fit
```{r}
mlp_final_wfemi = mlp_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_mlp_wfemi = bind_cols(
  df_test,
  predict(mlp_final_wfemi, df_test),
  predict(mlp_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_mlp_wfemi$bs =  (df_mlp_wfemi$.pred_1 - df_mlp_wfemi$INFEC_after_diag_num)^2
mean(df_mlp_wfemi$bs)
```

## create calibration plot
```{r}
mlp_cp = classifierplots::calibration_plot(df_mlp_wfemi$INFEC_after_diag, df_mlp_wfemi$.pred_1)
mlp_cp
```

## Bagged trees
### extract models from last fit
```{r}
bag_final_wfemi = bag_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_bag_wfemi = bind_cols(
  df_test,
  predict(bag_final_wfemi, df_test),
  predict(bag_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score000
```{r}
df_bag_wfemi$bs =  (df_bag_wfemi$.pred_1 - df_bag_wfemi$INFEC_after_diag_num)^2
mean(df_bag_wfemi$bs)
```

## create calibration plot
```{r}
bag_cp = classifierplots::calibration_plot(df_bag_wfemi$INFEC_after_diag, df_bag_wfemi$.pred_1)
bag_cp
```

## Decision tree
### extract models from last fit
```{r}
dec_final_wfemi = dec_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_dec_wfemi = bind_cols(
  df_test,
  predict(dec_final_wfemi, df_test),
  predict(dec_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_dec_wfemi$bs =  (df_dec_wfemi$.pred_1 - df_dec_wfemi$INFEC_after_diag_num)^2
mean(df_dec_wfemi$bs)
```

## create calibration plot
```{r}
dec_cp = classifierplots::calibration_plot(df_dec_wfemi$INFEC_after_diag, df_dec_wfemi$.pred_1)
dec_cp
```

## MARS
### extract models from last fit
```{r}
mars_final_wfemi = mars_last_fit_wfemi %>%
  pluck(".workflow", 1)


df_mars_wfemi = bind_cols(
  df_test,
  predict(mars_final_wfemi, df_test),
  predict(mars_final_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_mars_wfemi$bs =  (df_mars_wfemi$.pred_1 - df_mars_wfemi$INFEC_after_diag_num)^2
mean(df_mars_wfemi$bs)
```

## create calibration plot
```{r}
mars_cp = classifierplots::calibration_plot(df_mars_wfemi$INFEC_after_diag, df_mars_wfemi$.pred_1)
mars_cp
```

## Stack
### extract models from last fit
```{r}
df_cll_st_wfemi = bind_cols(
  df_test,
  predict(cll_st_wfemi, df_test),
  predict(cll_st_wfemi, df_test, type="prob")
)
```

## calculate Brier score
```{r}
df_cll_st_wfemi$bs =  (df_cll_st_wfemi$.pred_1 - df_cll_st_wfemi$INFEC_after_diag_num)^2
mean(df_cll_st_wfemi$bs)
```

## create calibration plot
```{r}
cll_st_cp = classifierplots::calibration_plot(df_cll_st_wfemi$INFEC_after_diag, df_cll_st_wfemi$.pred_1)
cll_st_cp
```


lr_clp = classifierplots::classifierplots(df_lr_wfemi$INFEC_after_diag, df_lr_wfemi$.pred_1)
lr_clp

dir.create("plot_wfemi_lr")
classifierplots::classifierplots_folder(
  test.y=df_lr_wfemi$INFEC_after_diag,
  pred.prob = df_lr_wfemi$.pred_1,
  folder = "plot_wfemi_lr",
  height = 250,
  width = 250
)

rf_clp = classifierplots::classifierplots(df_rf_wfemi$INFEC_after_diag, df_rf_wfemi$.pred_1)
rf_clp

dir.create("plot_wfemi_rf")
classifierplots::classifierplots_folder(
  test.y=df_rf_wfemi$INFEC_after_diag,
  pred.prob = df_rf_wfemi$.pred_1,
  folder = "plot_wfemi_rf",
  height = 250,
  width = 250
)

svm_clp = classifierplots::classifierplots(df_svm_wfemi$INFEC_after_diag, df_svm_wfemi$.pred_1)
svm_clp

dir.create("plot_wfemi_svm")
classifierplots::classifierplots_folder(
  test.y=df_svm_wfemi$INFEC_after_diag,
  pred.prob = df_svm_wfemi$.pred_1,
  folder = "plot_wfemi_svm",
  height = 250,
  width = 250
)

svm_r_clp = classifierplots::classifierplots(df_svm_r_wfemi$INFEC_after_diag, df_svm_r_wfemi$.pred_1)
svm_r_clp

dir.create("plot_wfemi_svm_r")
classifierplots::classifierplots_folder(
  test.y=df_svm_r_wfemi$INFEC_after_diag,
  pred.prob = df_svm_r_wfemi$.pred_1,
  folder = "plot_wfemi_svm_r",
  height = 250,
  width = 250
)

xgb_clp = classifierplots::classifierplots(df_xgb_wfemi$INFEC_after_diag, df_xgb_wfemi$.pred_1)
xgb_clp

dir.create("plot_wfemi_xgb")
classifierplots::classifierplots_folder(
  test.y=df_xgb_wfemi$INFEC_after_diag,
  pred.prob = df_xgb_wfemi$.pred_1,
  folder = "plot_wfemi_xgb",
  height = 250,
  width = 250
)

nb_clp = classifierplots::classifierplots(df_nb_wfemi$INFEC_after_diag, df_nb_wfemi$.pred_1)
nb_clp

dir.create("plot_wfemi_nb")
classifierplots::classifierplots_folder(
  test.y=df_nb_wfemi$INFEC_after_diag,
  pred.prob = df_nb_wfemi$.pred_1,
  folder = "plot_wfemi_nb",
  height = 250,
  width = 250
)

mlp_clp = classifierplots::classifierplots(df_mlp_wfemi$INFEC_after_diag, df_mlp_wfemi$.pred_1)
mlp_clp

dir.create("plot_wfemi_mlp")
classifierplots::classifierplots_folder(
  test.y=df_mlp_wfemi$INFEC_after_diag,
  pred.prob = df_mlp_wfemi$.pred_1,
  folder = "plot_wfemi_mlp",
  height = 250,
  width = 250
)

bag_clp = classifierplots::classifierplots(df_bag_wfemi$INFEC_after_diag, df_bag_wfemi$.pred_1)
bag_clp


dir.create("plot_wfemi_bag")
classifierplots::classifierplots_folder(
  test.y=df_bag_wfemi$INFEC_after_diag,
  pred.prob = df_bag_wfemi$.pred_1,
  folder = "plot_wfemi_bag",
  height = 250,
  width = 250
)
dec_clp = classifierplots::classifierplots(df_dec_wfemi$INFEC_after_diag, df_dec_wfemi$.pred_1)
dec_clp

dir.create("plot_wfemi_dec")
classifierplots::classifierplots_folder(
  test.y=df_dec_wfemi$INFEC_after_diag,
  pred.prob = df_dec_wfemi$.pred_1,
  folder = "plot_wfemi_dec",
  height = 250,
  width = 250
)

mars_clp = classifierplots::classifierplots(df_mars_wfemi$INFEC_after_diag, df_mars_wfemi$.pred_1)
mars_clp

dir.create("plot_wfemi_mars")
classifierplots::classifierplots_folder(
  test.y=df_mars_wfemi$INFEC_after_diag,
  pred.prob = df_mars_wfemi$.pred_1,
  folder = "plot_wfemi_mars",
  height = 250,
  width = 250
)

cll_st_clp = classifierplots::classifierplots(df_cll_st_wfemi$INFEC_after_diag, df_cll_st_wfemi$.pred_1)
cll_st_clp

dir.create("plot_wfemi_cll_st")
classifierplots::classifierplots_folder(
  test.y=df_cll_st_wfemi$INFEC_after_diag,
  pred.prob = df_cll_st_wfemi$.pred_1,
  folder = "plot_wfemi_cll_st",
  height = 250,
  width = 250
)

