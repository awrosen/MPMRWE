---
title: "ReproducebleExampleRasmus"
output: html_document
---
Load packages for project
```{r setup, include=FALSE}
options(width = 120)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(skimr)
library(tidymodels)
library(matrixStats)
library(vip)
#library(kernlab)
#library(doParallel)
library(discrim)
library(stacks)
#library(kknn)
```


# Loading data and data curation
```{r load_data}
## Synthetic data for analysis
dfraw <- read_csv("/users/home/candwei/pm_master/cll_tim_compound_infec_n.csv") %>% 
  rename("id" = "...1") 
```

```{r}
df <- dfraw %>% 
  select(-c(id, Patient_id, INFEC_days_after_diag)) %>% 
  mutate(INFEC_after_diag = factor(INFEC_after_diag, levels = c("1", "0")))
```

```{r}
set.seed(42)

# Create random samples of the rows with 20% set aside for testing
df_split <- df %>% 
  initial_split(prop = 0.8)

# Extract the datasets
df_other <- training(df_split)
df_test  <- testing(df_split)

# For the non testing set we create a 75/25 train/validation split
# The original data is then split into 60/20/20 proportions for training, validation and testing
df_val <- vfold_cv(df_other, v = 5)
```

# Define models

## Set overall recepi
```{r}
metric <- metric_set(roc_auc)

ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()
ctrl_bayes = control_stack_bayes()

cll_rec <-
  recipe(INFEC_after_diag ~ ., data = df_other) %>%
  step_zv(all_predictors()) %>%                # Remove variables with no variation
  step_impute_median(all_predictors()) %>%     # Impute missing values with median from training set
  step_normalize(all_predictors()) 

cll_wflow <-
  workflow() %>%
  add_recipe(cll_rec)
```


## EN
```{r}
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>% 
  set_mode("classification") %>%
  set_engine("glmnet")

lr_wflow <-
  cll_wflow %>%
  add_model(lr_mod)

lr_params <- 
  dials::parameters(
    penalty(),
    mixture()
  )

lr_grid <- 
  dials::grid_max_entropy(
    lr_params, 
    size = 3
  )

lr_res <- 
  tune_grid(
  object = lr_wflow,
  resamples = df_val,
  grid = lr_grid,
  control = ctrl_grid,
  metrics = metric
  )
```

We can print the metrics from the model training using the collect_metrics command.
```{r}
lr_res %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
lr_plot <- lr_res %>% 
  collect_metrics() %>% 
  mutate(mixture = as.factor(mixture)) %>% 
  ggplot(aes(x = penalty, y = mean, color = mixture, group = mixture),show.legend = F) + 
  geom_point(show.legend = F) + 
  geom_line(show.legend = F) + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 

# A similar plot is achieved using autoplot(lr_res),
# but it is demonstrated manually for learning purposes
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

lr_best

# Alternatively the sorting and slicing may be done using
# show_best(lr_res)
```

Finally we can plot the ROC for the best model
```{r}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Elastic Net")

autoplot(lr_auc)
```

## RF
```{r}
cores <- parallel::detectCores()
rf_mod <-
  rand_forest(
    mtry = tune(),
    min_n = tune(),
    trees = tune(),
  ) %>% set_mode("classification") %>%
  set_engine("ranger", num.threads = cores)

rf_wflow <-
  cll_wflow %>%
  add_model(rf_mod)

cll.mtry = select(df,-INFEC_after_diag)
finalize(mtry(),cll.mtry)

rf_params <- 
  dials::parameters(
    dials::mtry(),
    min_n(),
    trees()
  )

rf_params <-
  rf_params %>%
  update(mtry = finalize(mtry(), df %>% select(-INFEC_after_diag)))

rf_grid <- 
  dials::grid_max_entropy(
    rf_params, 
    size = 3
  )

rf_res <- 
  tune_grid(
    object = rf_wflow, 
    resamples = df_val, 
    grid = rf_grid,
    control = ctrl_grid,
    metrics = metric
    
  )
```

Print the top performing models
```{r}
show_best(rf_res)
```

We can plot the performance against the tuning parameters
```{r}
autoplot(rf_res)
```

ROC for the best model
```{r}
rf_best <- 
  rf_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  slice(1)

rf_best

rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Random forrest")

autoplot(rf_auc)
```


## SVM
```{r}
svm_mod <-
  svm_poly(
    cost  = tune(),
    degree = tune(),
  ) %>% set_mode("classification") %>%
  set_engine("kernlab")

svm_wflow <-
  cll_wflow %>%
  add_model(svm_mod)


svm_params <- 
  dials::parameters(
    cost(),
    degree()
  )

svm_grid <- 
  dials::grid_max_entropy(
    svm_params, 
    size = 3
  )
svm_grid$degree = as.integer(svm_grid$degree)

svm_res <- 
  tune_grid(
    object = svm_wflow, 
    resamples = df_val, 
    grid = svm_grid,
    control = ctrl_grid,
    metrics = metric
  )
```

We can print the metrics from the model training using the collect_metrics command.
```{r}
svm_res %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(svm_res)
# A similar plot is achieved using autoplot(lr_res),
# but it is demonstrated manually for learning purposes
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
svm_best <- 
  svm_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#svm_best
show_best(svm_res)
# Alternatively the sorting and slicing may be done using
# show_best(lr_res)
```

Finally we can plot the ROC for the best model
```{r}
svm_auc <- 
  svm_res %>% 
  collect_predictions(parameters = svm_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "SVM")

autoplot(svm_auc)
```

## SVM with radial basis
```{r}
svm_r_mod <-
  svm_rbf(
    cost  = tune(),
    rbf_sigma = tune(),
  ) %>% set_mode("classification") %>%
  set_engine("kernlab")

svm_r_wflow <-
  cll_wflow %>%
  add_model(svm_r_mod)


svm_r_params <- 
  dials::parameters(
    cost(),
    rbf_sigma()
  )

svm_r_grid <- 
  dials::grid_max_entropy(
    svm_r_params, 
    size = 3
  )

svm_r_res <- 
  tune_grid(
    object = svm_r_wflow, 
    resamples = df_val, 
    grid = svm_r_grid,
    control = ctrl_grid,
    metrics = metric
  )
```
We can print the metrics from the model training using the collect_metrics command.
```{r}
svm_r_res %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(svm_r_res)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
svm_r_best <- 
  svm_r_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#svm_best
show_best(svm_r_res)
# Alternatively the sorting and slicing may be done using
# show_best(lr_res)
```

Finally we can plot the ROC for the best model
```{r}
svm_r_auc <- 
  svm_r_res %>% 
  collect_predictions(parameters = svm_r_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "SVM radial")

autoplot(svm_r_auc)
```

## extrem gradient boost
```{r}
xgb_mod <-
  boost_tree(
    trees = tune(),
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune()) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_wflow <-
  cll_wflow %>%
  add_model(xgb_mod)


xgb_params <- 
  dials::parameters(
    trees(),
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction()
  )

xgb_grid <- 
  dials::grid_max_entropy(
    xgb_params, 
    size = 5
  )

xgb_res <- 
  tune_grid(
    object = xgb_wflow, 
    resamples = df_val, 
    grid = xgb_grid,
    control = ctrl_grid,
    metrics = metric
  )
```

We can print the metrics from the model training using the collect_metrics command.
```{r}
xgb_res %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(xgb_res)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
xgb_best <- 
  xgb_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#svm_best
show_best(xgb_res)
```

Finally we can plot the ROC for the best model
```{r}
xgb_auc <- 
  xgb_res %>% 
  collect_predictions(parameters = xgb_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "xgboost")

autoplot(xgb_auc)
```


## Multilayer perceptron
```{r}
mlp_mod <- 
    mlp(
      hidden_units = tune(),
      penalty = tune(),
      epochs = tune()
      ) %>% 
  set_engine("nnet", trace = 0,MaxNWts=2000) %>% 
  set_mode("classification")

mlb_wflow <-
  cll_wflow %>%
  add_model(mlp_mod)


mlp_params <- 
  dials::parameters(
    hidden_units(),
    penalty(),
    epochs()
  )

mlp_grid <- 
  dials::grid_max_entropy(
    mlp_params, 
    size = 3
  )

mlp_res <- 
  tune_grid(
    object = mlp_wflow, 
    resamples = df_val, 
    grid = mlp_grid,
    control = ctrl_grid,
    metrics = metric
  )
```

We can print the metrics from the model training using the collect_metrics command.
```{r}
mlp_res %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(mlp_res)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
mlp_best <- 
  mlp_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#mlp_best
show_best(mlp_res)
```

Finally we can plot the ROC for the best model
```{r}
mlp_auc <- 
  mlp_res %>% 
  collect_predictions(parameters = mlp_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Multilayer Perceptron")

autoplot(mlp_auc)
```

## Naive Bayes
```{r}
nb_mod <- 
  naive_Bayes(
    smoothness = tune(),
    Laplace = tune()
    ) %>% 
  set_engine("naivebayes") %>%
  set_mode("classification")

nb_wflow <-
  cll_wflow %>%
  add_model(nb_mod)


nb_params <- 
  dials::parameters(
    smoothness(),
    Laplace()
  )

nb_grid <- 
  dials::grid_max_entropy(
    nb_params, 
    size = 5
  )

nb_res <- 
  tune_grid(
    object = nb_wflow, 
    resamples = df_val, 
    grid = nb_grid,
    control = ctrl_grid,
    metrics = metric
  )
```

```{r}
nb_res %>% 
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(nb_res)
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
nb_best <- 
  nb_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

show_best(nb_res)
```

Finally we can plot the ROC for the best model
```{r}
nb_auc <- 
  nb_res %>% 
  collect_predictions(parameters = nb_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "naive Bayes")

autoplot(nb_auc)
```

# Bagged tree
```{r}
bag_mod <- 
  baguette::bag_tree(
           min_n = tune(),
           class_cost = tune()) %>% 
  set_engine("C5.0") %>%
  set_mode("classification")

bag_wflow <-
  cll_wflow %>%
  add_model(bag_mod)


bag_params <- 
  dials::parameters(
    min_n(),
    baguette::class_cost()
  )

bag_grid <- 
  dials::grid_max_entropy(
    bag_params, 
    size = 3
  )

bag_res <- 
  tune_grid(
    object = bag_wflow, 
    resamples = df_val, 
    grid = bag_grid,
    control = ctrl_grid,
    metrics = metric
  )
```


We can print the metrics from the model training using the collect_metrics command.
```{r}
bag_res %>%
  collect_metrics()
```

We can also plot the performance of all the models as a function of the tuning parameters, to get a quick overview.
```{r}
autoplot(bag_res)

# A similar plot is achieved using autoplot(lr_res),
# but it is demonstrated manually for learning purposes
```

We can select the best model, i.e. combination of penalty and mixture with best AUC, by sorting on the mean AUC value, and "slicing" of the first row.
```{r}
bag_best <- 
  bag_res %>% 
  collect_metrics() %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

#svm_best
show_best(bag_res)
# Alternatively the sorting and slicing may be done using
# show_best(lr_res)
```

Finally we can plot the ROC for the best model
```{r}
bag_auc <- 
  bag_res %>% 
  collect_predictions(parameters = bag_best) %>% 
  roc_curve(INFEC_after_diag, .pred_1) %>% 
  mutate(model = "Bagged trees")

autoplot(bag_auc)
```
# Last model
This section demonstrates how to retrain the model on the combined training and validation set, using the optimal tuning parameters, and evaluating the performance on the test set.

## LR
```{r}
# Create model with best parameter
lr_final_mod <- logistic_reg(penalty = lr_best$penalty, mixture = lr_best$mixture) %>% 
  set_engine("glmnet")

# Create final workflow
lr_final_workflow <- workflow() %>% 
 add_model(lr_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
lr_last_fit <- last_fit(lr_final_workflow, split = df_split)
lr_last_fit %>% collect_metrics()
```

We can print the most important features used for the prediction using the variable importance function. Vip depends on the type of model, so we include the type of variable importance score. For the EN model, which is a paramatric model, we get the model coefficients.
```{r}
lr_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```

## RF
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
rf_final_mod <- 
  rand_forest(mtry = rf_best$mtry, min_n = rf_best$min_n, trees = rf_best$trees) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# Create final workflow
rf_final_workflow <- workflow() %>% 
 add_model(rf_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
rf_last_fit <- last_fit(rf_final_workflow, split = df_split)
rf_last_fit %>% collect_metrics()
```

```{r}
rf_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```


## SVM
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
svm_final_mod <- 
  svm_poly(cost = svm_best$cost, degree = svm_best$degree) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

# Create final workflow
svm_final_workflow <- workflow() %>% 
  add_model(svm_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
svm_last_fit <- last_fit(svm_final_workflow, split = df_split)
svm_last_fit %>% collect_metrics()
```

```{r}
svm_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```

## SVM with radial basis
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
svm_r_final_mod <- 
  svm_rbf(cost = svm_r_best$cost, rbf_sigma = svm_r_best$rbf_sigma) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

# Create final workflow
svm_r_final_workflow <- workflow() %>% 
 add_model(svm_r_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
svm_r_last_fit <- last_fit(svm_r_final_workflow, split = df_split)
svm_r_last_fit %>% collect_metrics()
```

```{r}
svm_r_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```

## XGBOOSTING
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
xgb_final_mod <- 
  boost_tree(min_n = xgb_best$min_n,tree_depth = xgb_best$tree_depth, learn_rate = xgb_best$learn_rate,loss_reduction = xgb_best$loss_reduction, trees=xgb_best$trees) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

# Create final workflow
xgb_final_workflow <- workflow() %>% 
 add_model(xgb_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
xgb_last_fit <- last_fit(xgb_final_workflow, split = df_split)
xgb_last_fit %>% collect_metrics()
```

```{r}
xgb_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```

## mlp
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
mlp_final_mod <- 
  mlp(hidden_units = mlp_best$hidden_units, penalty = mlp_best$penalty, epochs = mlp_best$epochs) %>% 
  set_engine("nnet", trace=0) %>% 
  set_mode("classification")

# Create final workflow
mlp_final_workflow <- workflow() %>% 
 add_model(mlp_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
mlp_last_fit <- last_fit(mlp_final_workflow, split = df_split)
mlp_last_fit %>% collect_metrics()
```

```{r}
mlp_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```
## Naive Bayes
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
bag_final_mod <- 
   baguette::bag_tree(
           min_n = bag_best$min_n,
           class_cost = bag_best$class_cost)%>% 
  set_engine("C5.0") %>% 
  set_mode("classification")


# Create final workflow
bag_final_workflow <- workflow() %>% 
 add_model(bag_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
bag_last_fit <- last_fit(bag_final_workflow, split = df_split)
bag_last_fit %>% collect_metrics()
```

```{r}
bag_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```

## Naive Bayes
```{r}
# Create model with best parameter
# In order to extract the variable importance we need to specify it in set_engine
nb_final_mod <- 
  naive_Bayes(smoothness = nb_best$smoothness, Laplace = nb_best$Laplace) %>% 
  set_engine("naivebayes") %>% 
  set_mode("classification")


# Create final workflow
nb_final_workflow <- workflow() %>% 
 add_model(nb_final_mod) %>% 
  add_recipe(cll_rec)

# Last fit
nb_last_fit <- last_fit(nb_final_workflow, split = df_split)
nb_last_fit %>% collect_metrics()
```

```{r}
nb_last_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip(num_features = 20, include_type = T)
```

# Create stack
## Create stack
```{r}
cll_st <- 
  stacks() %>%
  add_candidates(lr_res) %>%
  add_candidates(rf_res) %>%
  add_candidates(svm_res) %>%
  add_candidates(svm_r_res) %>%
  add_candidates(xgb_res) %>%
  add_candidates(nb_res) %>%
  add_candidates(mlp_res) %>%
  add_candidates(bag_res) %>%
  blend_predictions() %>%
  fit_members()

```

## Visualize trade-off between number of members and performance
```{r}
autoplot(cll_st)
```

## Other visualization showing trade-off vs performance
```{r}
autoplot(cll_st, type = "members")
```

## Look at penalty 
```{r}
autoplot(cll_st, type="weights")
```

## Check parameters
```{r}
collect_parameters(cll_st,"lr_res")
```

## Predicting new data
```{r}
cll_pred <-
  df_test %>%
  bind_cols(predict(cll_st,.,type = "prob"))
  
yardstick::roc_auc(
  cll_pred,
  truth = INFEC_after_diag,
  contains(".pred_1")
)
```

## compare with individual models
```{r}
cll_pred <-
  df_test %>%
  select(INFEC_after_diag) %>%
  bind_cols(
    predict(
      cll_st,
      df_test,
      type = "class",
      members = TRUE
    )
  )
```

## compare members with total
```{r}
map_dfr(
  setNames(colnames(cll_pred),colnames(cll_pred)),
  ~mean(cll_pred$INFEC_after_diag == pull(cll_pred, .x))
) %>%
  pivot_longer(c(everything(),-INFEC_after_diag))
```
